\section{parameter optimization}
\label{sec:parameter_optimization}
The selection of the architacture of the model in the previous section results an a large set of architecture specific hyperparameters, for which an optimal value will be searched in rthis section. Table \ref{tab:hyperparameters} lists the hyperparameters that will be optimized in this section, together with a short explanation, in which architecture they occur out of the selected group and the range of values that will be searched.
\\ \\
\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\hline
\multicolumn{5}{|c|}{\textbf{Model Presence}} \\
\hline
Hyperparameter & Explanation & LSTM & Sequential & Range of values \\
\hline
\hline
batch size & The number of samples that will be used in each iteration & x & x & 32, 64, 128, 256, 512 \\
\hline
learning rate & The step size at each iteration & x & x & 0.0001, 0.001, 0.01, 0.1 \\
\hline
dropout rate & The fraction of the input units to drop & x & x & 0.1, 0.2, 0.3, 0.4, 0.5 \\
\hline
number of units & The number of units in the LSTM layer & x & & 32, 64, 128, 256, 512 \\
\hline
number of layers & The number of LSTM layers & x & & 1, 2, 3, 4, 5 \\
\hline
\hline
\end{tabular}
Since the possible values for each hyperparameter are infinite and it takes an enormous amount of time to evaluate all possible combinations, a literature research \cite[text]{keylist} was performed to find the most common values for each hyperparameter. These values will be used as the range of values that will be searched. 
\\ \\
Using a combination of KerasRegressor, which is a wrapper for the Keras library that allows the use of Keras models in scikit-learn, and both Grid- and RandomizedSearchCV, the model is trained and evaluated for each combination of hyperparameters. This is done by using KFold cross validation which splits the initial training data into a training and validation set in a randomized manner and which returns the mean squared error for each fold, indicating the performance of the model for that specific combination. The combination of hyperparameters that results in the best performance, having the lowest mean squared error, is selected as the optimal set of hyperparameters. 
\\ \\
As an example, Figure \ref{fig:gridsearch} shows the results of a grid search for the hyperparameters batch size and learning rate. The mean squared error is plotted against the batch size and learning rate. In this case, the combination of hyperparameters that results in the lowest mean squared error is a batch size of 32 and a learning rate of 0.001.
\\ \\
As even the evaluation of the narrowed combinations of hyperparameters takes a lot of time, the evaluation of all possible combinations of hyperparameters is not feasible. Therefore, initially a random search is performed. This means that for each hyperparameter, a random value is selected out of each range. This process is iterated for a predefined number of iterations. The results of the random search are plotted in Figure \ref{fig:randomsearch}. And had a mean squared error of 0.0001. After that, GridSearchCV is performed, with which hyperparameters were evaluated around the best hyperparameters found in the random search. In this way, it is possible to refine and assure that the found values are really quasi optimal. 
\\ \\
Nevertheless, an equilibrium between the optimality of the hyperparameters and the time it takes to evaluate them had to be found. As can be seen in Figure \ref{fig:performance_improvement}, the improvement in performance reduces drastically with a longer running time. Therefore, the optimal values were set as the values found after a search of 100 combinations. The results of the grid search are plotted in Figure \ref{fig:gridsearch}. Finally the best hyperparameters for each architecture are shown in Table \ref{tab:best_hyperparameters}.

\begin{figure} [H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/performance_improvement.png}
    \caption{Performance improvement over time}
    \label{fig:performance_improvement}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/gridsearch.png}
\caption{Grid search for the hyperparameters batch size and learning rate}
\label{fig:gridsearch}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/randomsearch.png}
\caption{Random search for the hyperparameters batch size and learning rate}
\label{fig:randomsearch}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\hline
\multicolumn{4}{|c|}{\textbf{Best Hyperparameters}} \\
\hline
Hyperparameter & LSTM & Sequential \\
\hline
\hline
batch size & 32 & 32 \\
\hline
learning rate & 0.001 & 0.001 \\
\hline
dropout rate & 0.1 & 0.1 \\
\hline
number of units & 128 & \\
\hline
number of layers & 1 & \\
\hline
\hline
\end{tabular}
\caption{Best hyperparameters for each architecture}
\label{tab:best_hyperparameters}
\end{table}

\\ \\
Using these quasi-optimal hyperparameters, the model is trained on the training data and evaluated on the test data. The results are shown in the next section.